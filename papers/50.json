{"metadata": {"Author": "Ganesh Rao (Cepha Imaging INDIA) 4866 2001 Oct 23 13:15:14", "Content-Type": "application/pdf", "Creation-Date": "2010-08-28T10:54:21Z", "Last-Modified": "2022-07-12T17:35:00Z", "Last-Save-Date": "2022-07-12T17:35:00Z", "X-Parsed-By": ["org.apache.tika.parser.DefaultParser", "org.apache.tika.parser.pdf.PDFParser"], "X-TIKA:content_handler": "ToTextContentHandler", "X-TIKA:embedded_depth": "0", "X-TIKA:parse_time_millis": "94", "access_permission:assemble_document": "true", "access_permission:can_modify": "true", "access_permission:can_print": "true", "access_permission:can_print_degraded": "true", "access_permission:extract_content": "true", "access_permission:extract_for_accessibility": "true", "access_permission:fill_in_form": "true", "access_permission:modify_annotations": "true", "cp:subject": "TeX output 2010.08.28:1053", "created": "2010-08-28T10:54:21Z", "creator": "Ganesh Rao (Cepha Imaging INDIA) 4866 2001 Oct 23 13:15:14", "date": "2022-07-12T17:35:00Z", "dc:creator": "Ganesh Rao (Cepha Imaging INDIA) 4866 2001 Oct 23 13:15:14", "dc:format": "application/pdf; version=1.5", "dc:title": "G:\\bioinformatics\\Bioinfo-ECCB(suppl-26)\\btq376.dvi", "dcterms:created": "2010-08-28T10:54:21Z", "dcterms:modified": "2022-07-12T17:35:00Z", "meta:author": "Ganesh Rao (Cepha Imaging INDIA) 4866 2001 Oct 23 13:15:14", "meta:creation-date": "2010-08-28T10:54:21Z", "meta:save-date": "2022-07-12T17:35:00Z", "modified": "2022-07-12T17:35:00Z", "pdf:PDFVersion": "1.5", "pdf:charsPerPage": ["5271", "6161", "6283", "5381", "4154", "2517", "3822", "5846", "3144"], "pdf:docinfo:created": "2010-08-28T10:54:21Z", "pdf:docinfo:creator": "Ganesh Rao (Cepha Imaging INDIA) 4866 2001 Oct 23 13:15:14", "pdf:docinfo:creator_tool": "DVIPSONE 2.2.6  http://www.YandY.com", "pdf:docinfo:modified": "2022-07-12T17:35:00Z", "pdf:docinfo:producer": "Acrobat Distiller 6.0 (Windows); modified using iTextSharp 4.1.6 by 1T3XT", "pdf:docinfo:subject": "TeX output 2010.08.28:1053", "pdf:docinfo:title": "G:\\bioinformatics\\Bioinfo-ECCB(suppl-26)\\btq376.dvi", "pdf:encrypted": "false", "pdf:hasMarkedContent": "false", "pdf:hasXFA": "false", "pdf:hasXMP": "true", "pdf:unmappedUnicodeCharsPerPage": ["0", "0", "0", "0", "0", "0", "0", "6", "0"], "producer": "Acrobat Distiller 6.0 (Windows); modified using iTextSharp 4.1.6 by 1T3XT", "resourceName": "b'50.pdf'", "subject": "TeX output 2010.08.28:1053", "title": "G:\\bioinformatics\\Bioinfo-ECCB(suppl-26)\\btq376.dvi", "xmp:CreatorTool": "DVIPSONE 2.2.6  http://www.YandY.com", "xmpMM:DocumentID": "uuid:13ab9291-44bf-4182-b5d9-7de0e04e8a35", "xmpTPg:NPages": "9"}, "content": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nG:\\bioinformatics\\Bioinfo-ECCB(suppl-26)\\btq376.dvi\n\n\n[10:53 28/8/2010 Bioinformatics-btq376.tex] Page: i531 i531\u2013i539\n\nBIOINFORMATICS Vol. 26 ECCB 2010, pages i531\u2013i539doi:10.1093/bioinformatics/btq376\n\nNonlinear dimension reduction and clustering by Minimum\nCurvilinearity unfold neuropathic pain and tissue embryological\nclasses\nCarlo Vittorio Cannistraci1,2,3,4,5,\u2217, Timothy Ravasi1,5, Franco Maria Montevecchi3,\nTrey Ideker5 and Massimo Alessio2,\u2217\n1Red Sea Integrative Systems Biology Lab, Computational Bioscience Research Center, Division of Chemical and\nLife Sciences and Engineering, King Abdullah University for Science and Technology (KAUST), Jeddah, Kingdom of\nSaudi Arabia, 2Proteome Biochemistry, San Raffaele Scientific Institute, via Olgettina 58, 20132 Milan, 3Department\nof Mechanics, 4CMP Group, Microsoft Research, Politecnico di Torino, c/so Duca degli Abruzzi 24, 10129 Turin, Italy,\n5Department of Bioengineering and Department of Medicine, University of California, San Diego, 9500 Gilman Drive,\nLa Jolla, CA 92093 USA\n\nABSTRACT\n\nMotivation: Nonlinear small datasets, which are characterized by\nlow numbers of samples and very high numbers of measures, occur\nfrequently in computational biology, and pose problems in their\ninvestigation. Unsupervised hybrid-two-phase (H2P) procedures\u2014\nspecifically dimension reduction (DR), coupled with clustering\u2014\nprovide valuable assistance, not only for unsupervised data\nclassification, but also for visualization of the patterns hidden in\nhigh-dimensional feature space.\nMethods: \u2018Minimum Curvilinearity\u2019 (MC) is a principle that\u2014for\nsmall datasets\u2014suggests the approximation of curvilinear sample\ndistances in the feature space by pair-wise distances over their\nminimum spanning tree (MST), and thus avoids the introduction of\nany tuning parameter. MC is used to design two novel forms of\nnonlinear machine learning (NML): Minimum Curvilinear embedding\n(MCE) for DR, and Minimum Curvilinear affinity propagation (MCAP)\nfor clustering.\nResults: Compared with several other unsupervised and supervised\nalgorithms, MCE and MCAP, whether individually or combined in\nH2P, overcome the limits of classical approaches. High performance\nwas attained in the visualization and classification of: (i) pain patients\n(proteomic measurements) in peripheral neuropathy; (ii) human organ\ntissues (genomic transcription factor measurements) on the basis of\ntheir embryological origin.\nConclusion: MC provides a valuable framework to estimate\nnonlinear distances in small datasets. Its extension to large datasets\nis prefigured for novel NMLs. Classification of neuropathic pain\nby proteomic profiles offers new insights for future molecular and\nsystems biology characterization of pain. Improvements in tissue\nembryological classification refine results obtained in an earlier\nstudy, and suggest a possible reinterpretation of skin attribution as\nmesodermal.\nAvailability: https://sites.google.com/site/carlovittoriocannistraci/home\nContact: kalokagathos.agon@gmail.com; massimo.alessio@hsr.it\nSupplementary information: Supplementary data are available at\nBioinformatics online.\n\n\u2217To whom correspondence should be addressed.\n\n1 INTRODUCTION\n\n1.1 The machine learning perspective\nVisualization and discrimination as well as supervised and\nunsupervised classifications are widely employed in computational\nbiology for the investigation and analysis of patterns hidden in wet-\nlab data. In the literature, \u2018supervised classification\u2019 is frequently\nsimplified into \u2018classification\u2019, and \u2018unsupervised classification\u2019 into\n\u2018clustering\u2019 and this may give rise to misunderstanding. To avoid\nterminological ambiguity, \u2018classification\u2019 is adopted throughout this\narticle to describe the general task of sample group attribution, while\nthe issue of whether such attribution is supervised or unsupervised\nwill be specified as and when necessary.\n\nSupervised methods for feature selection and classification\npresent several pitfalls (Smialowski et al., 2009), and small\ndatasets make analysis problematic (Martella, 2006). Complications\nparticularly intensify when samples are nonlinearly related in\nthe high-dimensional feature space obtained from high-throughput\ngenomic and proteomic measures. When the aim is to classify a low\nnumber of samples characterized by a very large number of genes,\nproblems with parameter estimation may arise, and dimensional\nreduction followed by clustering (Martella, 2006) is a valuable\nresponse to this scenario. Principal component analysis (PCA)\nhas often been employed (Martella, 2006) in combination with a\nclustering algorithm that groups homogeneous classes on the basis\nof principal components, but this approach is insufficiently powerful\nto deal with nonlinear datasets. In this article, we describe the use of\nnonlinear hybrid-two-phase (H2P) unsupervised machine learning\n(ML) methodologies\u2014specifically dimension reduction (DR) in\nconjunction with clustering\u2014for the concurrent visualization and\nclassification of biological samples. Our aim is to address the issue\nof nonlinearity and to improve the classification accuracy of recently\nproposed small nonlinear datasets. The methodological innovation\nwe introduce is a principle called \u2018Minimum Curvilinearity\u2019 (MC),\nwhich is used as framework for two novel forms of nonlinear\nML (NML): Minimum Curvilinear embedding (MCE) for DR and\nMinimum Curvilinear affinity propagation (MCAP) for clustering.\nFor small datasets, the \u2018MC\u2019 principle suggests the estimation of\ncurvilinear (geodesic) distances between sample data points as pair-\nwise distances over their minimum spanning tree (MST) constructed\nin feature space.\n\n\u00a9 The Author(s) 2010. Published by Oxford University Press.\nThis is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/\nby-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.\n\nD\now\n\nnloaded from\n https://academ\n\nic.oup.com\n/bioinform\n\natics/article/26/18/i531/205645 by guest on 12 July 2022\n\n\n\n[10:53 28/8/2010 Bioinformatics-btq376.tex] Page: i532 i531\u2013i539\n\nC.V.Cannistraci et al.\n\nTo test efficacy of the proposed algorithms, we considered locally\nlinear embedding (LLE) (Roweis and Saul, 2000), the proposed\nMCE and four other unsupervised MLs for nonlinear DR and we\ncompared their ability to solve dataset nonlinearity. Furthermore,\nwe compared support vector machine (SVM), classical affinity\npropagation (AP) and the proposed MCAP for their ability to classify\nsamples projected in reduced feature space.\n\n1.2 Computational biology motivations\nH2P ML procedures are extensively employed for image processing\n(Lattner et al., 2004) and for other applications, including\nbioinformatics (Baldi and Brunak, 1998). A recent study by\nCannistraci et al. (2009; Ravasi et al., 2010), which analyzed\ngenomic transcription factor (TF) measurements, uncovered the\npresence of specific human tissue patterns. Based on nonlinear\nDR coupled to clustering in bi-dimensional reduced space, the\nmethod offered efficient data visualization and discrimination and,\nmore interestingly, achieved high accuracy in the unsupervised\nclassification of 32 human tissues, on the basis of their embryonic\norigin. Improvements obtained in the analysis of this dataset are\nshown in the last part of the article, in which several unsupervised\nH2P ML methods are compared. However, the main aim of this\narticle is to uncover insights and perspectives that in turn generate\nsolutions for real classification problems in medicine. Accordingly,\nthe first topic selected consists in the development of methods for\nthe classification of subjects with neuropathic pain, which is a major\nissue in translational and clinical medicine (Baron, 2006; Finnerup\nand Jensen, 2006). Specifically, we deal with peripheral neuropathy\nthat occurs either with or without pain. Interestingly, some of the\npatients without pain (NP) can, as the disease progresses, develop\na pathological variant with pain (P). Since current knowledge of\nmolecular disease mechanisms is poor, no single pain measure has\nsufficient reliability and validity. New integrative strategies for early\ndiagnosis could greatly enhance the timeliness of therapy planning,\nand interest in discovering reliable classification and prediction\nmethods for pain patients is accordingly considerable (Baron, 2006;\nFinnerup and Jensen, 2006; Meyer-Rosberg et al., 2001).\n\nCerebrospinal fluid (CSF) is a valuable source of information for\nbiologists and physicians. A recent computational study analyzed\na dataset of 2D electrophoresis (2DE) gel images derived from\nproteomic CSF profiles of peripheral neuropathic patients (Pattini\net al., 2008). Control (C) and Pain (P) groups were partially\nseparated (leave-one-out cross-validation accuracy 68.75%) by a\nnonlinear surface in the space of the first three principal components\nextracted by PCA. The discriminative characterization found for\npatients with pain, along with a further reasons, led us to reconsider\nthis dataset (n=23) (Pattini et al., 2008). The first additional reason\nwas our interest in assessing the efficiency of differing NML as\nsolutions for the nonlinearity revealed in the profile of pain subjects.\nParticularly, we tested whether it is possible to solve this nonlinearity\nby projecting the data in a reduced, 2D space. The result was a clear\nvisualization of proximity and separation between controls and pain\nsubjects, and a minimization of the problem faced by the classifier in\nfinding a line of separation in two dimensions. Our second incentive\nwas that we had the opportunity to follow disease progression;\nneuropathic patients were still under clinical observation, and four\nNP patients had developed the clinical features of neuropathic pain\n\n(P group). The third reason was the enlargement of the dataset sample\nto its current total of 42 individuals.\n\n2 DATA AND ALGORITHMS\n\n2.1 Dataset descriptions\nThe proteomic dataset was obtained from 2DE images generated\nfrom CSF samples. 2D gel generation was described in the original\nproteomics study (Conti et al., 2005). Each 2DE image was denoised\nby median modified wiener filter (MMWF) (Cannistraci et al.,\n2009) and spot detected by means of Progenesis PG240 v2006\nsoftware (Nonlinear dynamics, Newcastle, UK). Spot calibration\u2014\nin accordance with protein chemo-physical coordinates (isoelectric\npoint, pI; relative molecular mass, Mr)\u2014enabled the correction\nof spot location differences between differing gels. Spot volume\nwas estimated by means of its optical density (sum of the spot\npixels) normalized as a percentage of total spot optical density\nin the gel image (Pattini et al., 2008). From each image a vector\nof 2050 proteomic features was obtained by means of a strategy\npreviously developed, described in depth and validated by Pattini\net al. (2008). This dataset (dataset 1) was reduced from the original\n24 to 23 samples and divided into three groups: C =8, NP = 8, P=7.\nAs suggested by Pattini et al. (2008), we excluded the strongly noised\n2DE gel image corresponding to sample P7, which had been used in\nthe previous study exclusively as an internal check. The validation\nphase introduced a new proteomic dataset (M =19) which, together\nwith dataset 1, formed dataset 2. The new M samples derived from\na neurological study of amyotrophic lateral sclerosis (ALS) patients\nnot affected by neuropathic pain (Conti et al., 2008). The total\nnumber of subjects analyzed in dataset 2 of the current study is 42.\nThe demographic and clinical features of dataset 2 subjects/patients\nare shown in Supplementary Table S1. The dataset is provided on\nthe web site indicated in Section 5.3.\n\nThe dataset of human tissues (dataset 3) was provided as\nsupplementary material in the original paper (Ravasi et al., 2010)\nand consists of 32 human tissues and two monocyte cell lines.\nWe exclusively considered human tissues, because cell lines had\noriginally been introduced as an internal check.Atotal 1321 genomic\nTF measurements were considered.\n\n2.2 Layout of the neuropathic pain study\nA flux graph is provided in Supplementary Figure S1. It clarifies the\nsteps of the H2P procedure, which was used for feature reduction\nand supervised classification of the proteomic samples, as well as for\ncomparison with the unsupervised H2P variants explained at the end\nof this paragraph. The layout consists of two stages. The nonlinear\nmapping of the data in 2D reduced space requires the tuning of a free\nparameter k that occurs in some MLs for nonlinear dimensionality\nreduction. This parameter can vary between 1 and n\u22121 neighbors,\nwhere n is the dataset sample size, and is generally used to infer local\nand/or global manifold topology. Here, the idea is also to tune this\nparameter in order to offer DR projections that are more informative\nfor pain discrimination. The best tunings for LLE (Roweis and\nSaul, 2000), Gaussian kernel-PCA (KPCA) (Shawe-Taylor and\nCristianini, 2004), Local Tangent Space Analysis (LTSA) (Zhang\nand Zha, 2004) and Isomap (Tenenbaum et al., 2000) were learned\nin order to optimize assignment of subjects to the C and P samples.\nThis assignment, together with the comparison of ML performance\n\ni532\n\nD\now\n\nnloaded from\n https://academ\n\nic.oup.com\n/bioinform\n\natics/article/26/18/i531/205645 by guest on 12 July 2022\n\n\n\n[10:53 28/8/2010 Bioinformatics-btq376.tex] Page: i533 i531\u2013i539\n\nNonlinear dimension reduction and clustering by Minimun Curvilinearity\n\nin solving dataset nonlinearity, was accomplished in \u2018Stage 1\u2019. In the\ncomparison, a further NML was considered, namely Sammon multi-\ndimensional scaling (S-MDS), also known as Sammon Mapping\n(Sammon, 1969). A nonlinear MDS that preserves small distances\nbetween data points in the reduced space better than classical MDS,\nS-MDS is a parameter-free NML that accordingly does not require\ntuning. In addition, the proposed parameter-free NML called MCE\nwas considered, and its algorithm is presented in Section 2.5.\n\nMCE and LLE offered the best dimensionality reduction for linear\ndiscrimination of controls C and subjects with pain P, and they were\ntherefore selected for comparison in the reduction of feature space in\nthe second classification step. In particular, on the basis that LLE was\ntuned to preserve similarities in relation to the presence or absence\nof pain, it was also tested in combination with SVM (supervised\nH2P approach) for the classification of pain neuropathic patients.\n\nIn \u2018Stage 2\u2019design for validation, we propose a procedure in which\nthe SVM classifier is applied in the 2D feature space obtained by\nLLE; the free LLE parameter is fixed to the best value learned in\nStage 1. The SVM classifier also requires a training phase to learn\nthe decision rule used for sample supervised classification in the\nreduced, 2D feature space. The training of the manifold-NML helps\nto \u2018learn the similarities\u2019 related to the presence or absence of pain\nbetween samples in the high-dimensional feature space, and to map\nthe samples enhancing these similarities in a reduced feature space.\nIn contrast, SVM training helps to \u2018learn a rule for separation and\ndiscrimination of the samples\u2019 by exploiting the advantage that the\nsimilarities between close samples are enhanced in the new reduced\nfeature space. To ensure robustness, the training both in the \u2018tuning\nprocedure\u2019 and in the \u2018classification procedure\u2019 applied leave-one-\nout cross validation (LOOCV). For the LOOCV procedure, five of\nthe total eight C and four of the total seven P subjects were randomly\nselected and used as training exemplars. The dataset was ordered in\nthe following way: C1\u2212C5 and P1\u2212P4 were used as labels of the\ntraining data. The same data were used both for training of the NMLs\nin Stage 1 and for training of the SVM in Stage 2. The remaining\nsamples were randomly labeled C6\u2212C8 and P5\u2212P7, and used only\nfor validation.\n\nThe validation stage was divided into two tasks:\n\n(i) disease course in NP patients (n=8) was predicted as pain or\nno pain state; in addition, the subjects not used for training\n(C6, C7, C8 and P5, P6, P7) were also classified by SVM.\n\n(ii) the dataset was extended and the new control patients M\n(n=19) were classified as belonging to the pain or no pain\nstate.\n\nAs already mentioned, training a supervised classifier with a\nsmall number of samples (five controls versus four pain subjects)\nin order to infer a model is risky, and it could be further argued\nthat the use of SVM for classification in the reduced linearized\nfeature-extracted space is excessive for this purpose. To address\nthese points, we designed a second H2P approach, completely\nunsupervised, that substitutes the supervised classifier with an\nalgorithm for unsupervised classification (clustering). Statistical\nevaluation (accuracy, sensitivity, specificity, precision) of the SVM\nwas performed only on the testing samples and excluding the\nsamples used for training. The unsupervised classification (which\ndoes not require training samples) was in turn performed on the\nentire set of samples in the datasets.\n\n2.3 Minimum Curvilinearity\nMC principle has its starting point in the consideration that for\ndatasets of reduced size the idea of estimation or inference of\nmanifold topology in the feature space might be misleading due\nto the small number of samples. We speculate that in this case\nit might be more congruous to simply speak of estimation of\nnonlinear sample distances. MC is proposed as a way to estimate\nnonlinear sample distances by MST without any need for tuning\nparameters. A different, interesting principle, summarized in the\nphrase: \u2018think globally and fit locally\u2019, was introduced with LLE.\nExploiting local symmetries of linear reconstructions, LLE is able\nto learn the global structure of nonlinear manifolds (Roweis and\nSaul, 2000). This procedure, however, costs the introduction of\none free parameter for neighbourhood estimation, which can be a\npoint of weakness in unsupervised tasks. A recent study by Bogu\u00f1\u00e1\net al. (2009) on the navigability of complex networks found that a\ngeneral property is present in the hidden metric spaces of several\nartificial and biological networks. This property is dictated by the\nshape of the \u2018hidden metric space\u2019, which forces the system to\nform local interactions between subsets of its elements mapped\nin the \u2018observable network topology\u2019 as different sub-networks of\ninteracting nodes. On the other hand, the hidden space also guides\nthe greedy-routing process that connects nodes located in differing\nsub-networks. If this theory is adopted in the framework of our\nstudy\u2014applied to the sample representation in the hidden feature\nspace\u2014it offers a valid theoretical support for approaches such\nas \u2018think globally and fit locally\u2019 and or \u2018MC\u2019. Indeed, in small\ndatasets, MST provides a reasonably accurate map both of the\nlocal connection geometry of near and sub-network-related samples\n(nodes) and of the global connection geometry between samples\n(nodes) located in separated regions of the multi-dimensional space.\n\nMC suggests the estimation of curvilinear distances between\nsample data points in small datasets as pair-wise distances over\ntheir MST constructed in the feature space. The collection of all\nthese nonlinear pair-wise distances forms a distance matrix\u2014the\nMC-distance matrix\u2014to be used as an input in algorithms for DR\nor clustering.\n\n2.4 Minimum Curvilinear affinity propagation\nAlthough classical AP is a powerful algorithm for clustering that\nworks very well for regularly shaped clusters, with elongated or\nirregular multi-dimensional data it may force division of single\nclusters into separate ones or it may provide low-clustering results\n(Leone et al., 2007). The innovation we propose to solve these issues\nin elongated datasets is a clustering algorithm that runs AP (Frey\nand Dueck, 2007) over the MST of the samples (here represented\nin 2D-reduced space). We named this algorithm MCAP clustering.\nMore generally, MCAP is able to define clusters that pass messages\nbetween the samples that are nodes on the MST obtained in the multi-\ndimensional feature space. Taken from another perspective, we can\nsay that this algorithm is MST-guided: we estimate the curvilinear\ndistances between the samples distributed in feature space by MST\nand then, in accordance with a message passing procedure we send\nmessages between sample points following the preferential highway\ntracked by MST. Details of the algorithm are reported in Section 5.3.\n\nWe compare MCAP results with those offered by the classical AP\napproach (Frey and Dueck, 2007), where the similarities (negative\ndistances) between the samples are computed as negative squared\n\ni533\n\nD\now\n\nnloaded from\n https://academ\n\nic.oup.com\n/bioinform\n\natics/article/26/18/i531/205645 by guest on 12 July 2022\n\n\n\n[10:53 28/8/2010 Bioinformatics-btq376.tex] Page: i534 i531\u2013i539\n\nC.V.Cannistraci et al.\n\nerror distances (Frey and Dueck, 2007). For MCAP, the similarities\nare the negative values extracted from the MC-distance matrix. The\npreference parameter (Frey and Dueck, 2007) for both MCAP and\nAP is tuned to the value that offers two clusters as an algorithm\nresult.\n\n2.5 Minimum Curvilinear embedding\nIn terms of comparative ML theory, dimensional reduction and\nclustering are the two cornerstones of unsupervised learning\n(Ghahramani, 2004). We can therefore imagine an algorithm for DR\nthat is a distance-matrix analog of the MCAP clustering algorithm.\nThis nonlinear dimensionality reduction algorithm was named MCE\nand uses the MC-distance matrix as an input for the classical MDS\nor the nonlinear S-MDS. MCE algorithm details are provided in\nSection 5.4.\n\nMCE can be interpreted as the \u2018minimum curvilinear\u2019extension of\nMDS. The fact that MCE is a nonlinear (and curvilinear) extension\nof MDS represents a point of similarity with Isomap, which in\nturn is the manifold geodesic extension of MDS. Isomap computes\nsample geodesic distances over the manifold as shortest paths on\nthe neighborhood graph, as constructed on the bases of the first\nk Euclidian-distance neighbors, where k is the free parameter to\ntune. The principal weakness of Isomap is the algorithm instability\nencountered in embedding of manifolds with local nonlinearity or\ndiscontinuity (Balasubramanian and Schwartz 2002). With the low\nnumber of samples available for inferring manifold topology as\nour starting point, we argue that the strategy of global manifold\nreconstruction used by Isomap might be not congruous for small\nand irregular datasets.\n\n3 RESULTS AND DISCUSSION\n\n3.1 Data nonlinearity is successfully addressed\nFigure 1A shows algorithms\u2019 performance in linear discrimination\nbetween controls and pain subjects. Performance optimality was\nestimated by the maximization of a proposed index for tuning\nevaluation (TE). This index was evaluated: (i) for increasing values\nof k, which is the free parameter present in the manifold NML; (ii) for\nincreasing values of standard deviation, which is the free parameter\npresent in the kernel of the Gaussian KPCA. MCE and S-MDS were\nevaluated by the same index, but they do not present free parameter\nto tune. The TE index is computed by means of LOOCV in order to\nprevent overfitting. LOOCV is also used to estimate classification\naccuracy, as reported in Figure 1B. TE is evaluated as an average\nmeasure of linear separation obtained by the removal of one sample\nper LOOCV round and by the subsequent SVM estimation of\nthe margin of linear separation between the remaining samples.\nAccuracy is estimated as an average of classification successes\ncalculated by including the sample omitted in the LOOCV round,\nand by evaluating its label (control or pain) by means of the SVM\nseparation line. Details about TE index and accuracy evaluation are\nprovided in Section 5.1. Figure 1C summarizes the best performance\nfor each tested algorithm.\n\nData nonlinearity between C and P is successfully addressed\nduring tuning (Stage 1), where four out of six NMLs (LLE, MCE,\nIsomap, S-MDS) attained linear separation with an accuracy of 1\n(Fig. 1B, result not represented for S-MDS), and two out of six\n(KPCA and LTSA) attained linear separation with an accuracy\n\nFig. 1. Tuning and comparison to address the data nonlinearity. (A) TE\nfor LLE (yellow line), LTSA (blue line) and Isomap (green line). The x-axis\nreports different values of neighborhood parameter k; y-axis reports the index\nfor TE. (B) Classification accuracy for LLE (yellow line), LTSA (blue line)\nand Isomap (green line). The x-axis reports different values of neighborhood\nparameter k; y-axis reports values of accuracy. (C) Best performance in linear\ndiscrimination compared between tested algorithms.\n\nof 0.89 (eight successes in nine LOOCV rounds; the result for\nKPCA is not displayed in the figure). This demonstrates that\nlinearization is obtained as a result of generalized NML capacity,\nand that it is not related to an ability of a single algorithm.\nIn particular, LLE and MCE achieved the highest TE value and\nthey scored the best performance in linear discrimination (Fig. 1A\nand C). Surprisingly, MCE attained this result without tuning of\nany parameter and with a score of 1 on accuracy, while LLE was\nthe only manifold NML that enabled high-linear separation for\nlow numbers of neighbors\u2014in a range between 1 and 4\u2014where\nIsomap and LTSA failed (Fig. 1A and B). Isomap for small k\nvalues was not able to recover the manifold structure because the\nreconstruction of the neighborhood graph was not complete, and\nthis failure did not permit the embedding of the overall number of\nsamples. This weakness of Isomap is due to its topological instability\n(Balasubramanian and Schwartz 2002). Isomap may construct\nerroneous connections in the neighborhood graph, and such short-\ncircuits impair its performance (Balasubramanian and Schwartz\n2002). The fact that LTSA showed very low performance in the\nsame range where Isomap showed topological instability is a further\nconfirmation of local nonlinearity present in the dataset structure.\nThe locality of the problem is demonstrated by the fact that both\nalgorithms showed inefficiency using a small number of neighbors\nfor manifold reconstruction, and this confirms the result obtained in\nthe previous computational study (Pattini et al., 2008). The reasons\nwhy Isomap and LTSA show the same behavior, in contrast to\nLLE, which yields the best performances in this range, are to be\nfound in the differing hypotheses underlying ML applicability. Both\nIsomap and LTSA are sensitive to the assumption of local manifold\nlinearity (Zhang and Zha, 2004)\u2014which seems to be not satisfied\nby the dataset considered in our study\u2014whereas LLE provides\na local reconstruction that is less sensitive to this assumption.\nLLE preserves the local properties of the manifold by means of\na \u2018reconstruction weights operation\u2019, which locally linearizes\u2014by\nsolving a constrained least-squares problem\u2014the manifold in the\nneighborhood of each sample (Roweis and Saul, 2000). For datasets\nwith high-intrinsic dimensionality and low number of samples,\n\ni534\n\nD\now\n\nnloaded from\n https://academ\n\nic.oup.com\n/bioinform\n\natics/article/26/18/i531/205645 by guest on 12 July 2022\n\n\n\n[10:53 28/8/2010 Bioinformatics-btq376.tex] Page: i535 i531\u2013i539\n\nNonlinear dimension reduction and clustering by Minimun Curvilinearity\n\nFig. 2. Evaluation on dataset 1. (A) Result of AP clustering in the space of first two dimensions extracted by LLE. (B) Result of MCAP clustering in the\nspace of first two dimensions extracted by LLE. (C) Results of AP and MCAP clustering in the space of first two dimensions extracted by MCE. Blue line for\npain cluster, red line for control cluster. White skeleton in panel (B) indicates the partition generated by MCAP over the MST; link deleted by clustering was\nbetween samples P6 and NP3. SVM decision rule for classification (yellow line in panels A and B) is obtained considering the training samples (C1\u2212C5;\nP1\u2212P4). (D) Evaluation of SVM, MCAP and AP for pain classification in LLE reduced space. (E) Evaluation of MCAP and AP for pain classification in\nMCE reduced space.\n\nthe underlying estimation of the manifold might be difficult and\nhighly variable. Moreover, the local linearity assumption around\ncertain data points may be violated, at least anisotropically (i.e.\nonly in some manifold directions). Thus, techniques such as Isomap\nand LTSA may be less successful. In contrast, MCE\u2014designed to\nestimate nonlinear distances and to deal with local irregularity in\nsmall datasets\u2014addresses nonlinearity by considering even the total\ngroups of control (C, red spots) and pain (P, blue spots) patients\npresent in dataset 1 (Fig. 2C). LLE, as expected, attained comparable\nperformance in this task as well (Fig. 2A and B).\n\n3.2 Prediction and classification of pain subjects\nFigure 2 displays the results of different H2P procedures on dataset 1\nobtained by combining: (i) LLE with SVM, MCAP or AP; (ii) MCE\nwith MCAP or AP. Although LLE offered a clear linear separation\nover the first reduced dimension between pain and no-pain subjects,\nthe elongated shape in the bi-dimensional space of the no-pain group\ncaused the failure of AP to identify the right cluster attributions\n(Fig. 2A). This evidence was already reported in the literature\n(Leone et al., 2007). In contrast, MCAP succeeded in this task\n(Fig. 2B) because the message passing procedure was guided by\nthe MST skeleton (white skeleton, Fig. 2B). MCE too provided\nlinear separation over the first reduced dimension (Fig. 2C), but\nits embedded groups were more regular than those of LLE (Fig. 2A\nand B), this is why both AP and MCAP provided the same clustering\nin the MCE reduced space (Fig. 2C). The statistical evaluation\ndisplayed in Figure 2D and E suggests that MCE\u2013MCAP, which\nprovided the same result as LLE\u2013MCAP but without any tuning,\nenjoys high efficiency: a completely unbiased achievement.\n\nThis superiority was particularly evident in the second evaluation,\nperformed on dataset 2 (Fig. 3), in which the introduction of the\n\nnovel sample set M caused LLE to shift the linear separation between\npain and no-pain state from the first to the second dimension,\nwhile the first dimension became discriminative for the various\npathological states (Fig. 3A and B). Surprisingly, MCE was still\nable to discriminate the mixture of five different states over the\nfirst dimension (Fig. 3C, data and code to reproduce the figure are\nprovided at the link indicated in Section 5.3): on the left patients\naffected by ALS neuropathy (M); in the centre controls (C), while\nat the bottom-centered peripheral neuropathic patients without pain\n(NP); on the right, patients with peripheral neuropathy and pain (P,\nand NP with pain). The fact that MCE only needs the first dimension\nto offer a gradual and shaded landscape of this intricate scenario is\nimpressive, especially if we consider the simplicity of the principle\nbehind this NML, and the absence of parameters to tune. The result\nof the statistical evaluation displayed in Figure 3D and E suggests\nthat MCE\u2013MCAP provides superior unsupervised discrimination of\npain and no-pain subjects, which in turn shows that pain is the\nprevalent discrimination factor over the first MCE dimension. On\nthe other hand, the performance of the supervised H2P procedure\nconsisting in LLE\u2013SVM (Fig. 3A, B and D) proved to be robust\ndespite the introduction of new samples. However appraisal of this\nlast finding should be tempered by the fact that there were very few\ntest samples.\n\nFrom the biological point of view, our findings strongly support\nthe efforts to discover reliable methods for the classification of\nsubjects with pain, and encourages speculation about possible ways\nto distinguish the patients\u2019 states in relation to the proteomic pain\npattern hidden in their CSF. In order to advance any serious\nbiological claim, a further study with a larger dataset and a\ncongruous investigation of the relation between the significant\nfeatures is mandatory, yet this result is important because of\n\ni535\n\nD\now\n\nnloaded from\n https://academ\n\nic.oup.com\n/bioinform\n\natics/article/26/18/i531/205645 by guest on 12 July 2022\n\n\n\n[10:53 28/8/2010 Bioinformatics-btq376.tex] Page: i536 i531\u2013i539\n\nC.V.Cannistraci et al.\n\nFig. 3. Evaluation on dataset 2. (A) Result of AP clustering in the space of first two dimensions extracted by LLE. (B) Result of MCAP clustering in the space\nof first two dimensions extracted by LLE. (C) Results of MCAP clustering in the space of first two dimensions extracted by MCE. Blue line for pain cluster,\nred line for control cluster. White skeleton in panel (B) indicates the partition generated by MCAP over the MST; link deleted by clustering was between\nsamples C3 and NP3. SVM decision rule for classification (grey line in panel A and B) is obtained considering the training samples (C1\u2212C5; P1\u2212P4).\n(D) Evaluation of SVM, MCAP and AP for pain classification in LLE reduced space. (E) Evaluation of MCAP and AP for pain classification in MCE reduced\nspace.\n\nthe high-throughput proteomic screening approach employed to\ncharacterize every sample. An interesting final note from the\nclinical standpoint is that the unsupervised analysis provided\naccurate identification (only one misclassification NP3, out of a\ntotal eight NP samples) of future pain for NP patients (Figs 2A\u2013\nC and 3A\u2013C). This result is summarized in Supplementary Table\nS2 (see the \u2018computationally predicted state\u2019 column), together with\nthe clinical follow-up at 6\u201312 months and at >1 year (see \u2018follow\nup\u2019 columns).\n\n3.3 The tissue embryological classification is improved\nOn dataset 3, MCE and LLE (Fig. 4A and B) demonstrated the\nbest dimensionality reduction (same clustering accuracy, Fig. 4D)\nby solving the nonlinearity better than Gaussian KPCA (Fig. 4D),\nwhile PCA performance was much lower (Fig. 4C and D). Isomap\nsuffered from instability and its DR was not effective for evaluation.\n\nThe ability of MCE to provide a discriminative landscape where the\nsample classes are gradually unfolded along the first dimension is\nmaintained in this dataset too (Fig. 4A). In contrast, and as in the\nprevious evaluation, LLE, needs to combine the first and second\ndimensions for a complete discrimination of the classes (Fig. 4B).\nAs previously mentioned, this result in DR by MCE is very important\nbecause it is completely unbiased (absence of free parameter to tune\nin the algorithm). LLE allowed best clustering considering k =5\nboth for MCAP and AP, and this value was tuned on the basis\nof knowledge of the sample labels. Surprisingly, the results for\nMCE and LLE are not only similar in accuracy, but also in cluster\nshape and in the co-localization of differing samples, especially\nin the endodermal cluster (red color, Fig. 4A and B). We do not\nhave any biological explanation for the misclassification of lymph-\nnode (22, Fig. 4A and B), but it was suggested (Dorshkind, 2002)\nthat bone marrow (21, Fig. 4A and B) might also contain distinct\nendodermal progenitors capable of contributing to components of\n\ni536\n\nD\now\n\nnloaded from\n https://academ\n\nic.oup.com\n/bioinform\n\natics/article/26/18/i531/205645 by guest on 12 July 2022\n\n\n\n[10:53 28/8/2010 Bioinformatics-btq376.tex] Page: i537 i531\u2013i539\n\nNonlinear dimension reduction and clustering by Minimun Curvilinearity\n\nFig. 4. Evaluation on dataset 3. (A) Result of MCAP clustering in the space of first two dimensions extracted by MCE. (B) Result of MCAP clustering in\nthe space of first two dimensions extracted by LLE. (C) Results of MCAP clustering in the space of first two dimensions extracted by PCA. Green line for\nectodermal cluster attribution, yellow line for mesodermal cluster attribution, red line for endodermal cluster attribution. (D) Evaluation of MCAP and AP for\nunsupervised classification in reduced space obtained by different methods.\n\nthe gastrointestinal system such as liver (27 and 28, Fig. 4A and B).\nConversely, for MCE the mesodermal attribution of thyroid and\nsalivary-gland samples (23 and 24, Fig. 4A) seems to be an\nerror due to limitations of unsupervised classification rather than\nto a misleadingly low-dimensional localization. MCAP invariably\noffered better accuracy (Fig. 4D) than did AP, and thus confirmed\nthe results previously obtained on datasets 1 and 2. The most\nimpressive result is that the MCE\u2013MCAP H2P procedure achieved\n84% accuracy on the genomic TF expressions in a completely\nunsupervised manner. This is an improvement that supports the\nresult (82% accuracy) reported in the previous article (Ravasi\net al., 2010) by the small (only six interactions) TF-homeobox sub-\nnetwork, and confirms both the procedure\u2019s power in embryological\ndiscrimination and its potential importance in tissue differentiation\nprocesses.\n\nInterestingly, skin (labeled as ectodermal) was always classified\nin the mesodermal cluster in each of the different ML analyses (label\n8, Fig. 4A\u2013C). This is in accordance with the classification attained\nin the original article (Ravasi et al., 2010), but there interpreted\nas misclassification. In the light of the latest results, a possible\nmesodermal re-attribution of the skin label could be considered.\nThe biological explanation resides in the multi-layer structure of the\nskin: although the first layer of the skin (epidermis) is ectodermal, the\nextracted skin sample might also contain the second layer (dermis),\nwhich is of mesodermal origin.\n\n4 CONCLUSION AND FUTURE PERSPECTIVES\nMCE and LLE were very effective for DR because they allowed\nsimilarly high-clustering accuracy, and they occasionally uncovered\nanalogous geometry in sample localization (Fig. 4A and B).\n\nWe speculate that these similarities between MCE and LLE results\nare evidence of closeness\u2014as far as small datasets are concerned\u2014\nbetween the principles of \u2018MC\u2019 and \u2018think globally and fit locally\u2019\nthat respectively underlay MCE and LLE. Moreover, the fact that\nMCE only required the first dimension to completely unfold as many\nas five different classes (Fig. 3C) is striking, especially if we consider\nthe simplicity of the principle this NML is based upon, and the\nabsence of a parameter to tune. In our evaluations, LLE needed\nthe first and second dimensions to yield the same discriminative\nresults, and this comparable performance was obtained at the cost\nof a free parameter to tune. If no label hypothesis is provided (as\nwas the case for tissue embryological attribution, the uncovering\nof which was unsupervised), it is hard to imagine an unsupervised\nstrategy that indicates the right tuning for unfolding the classes\nhidden in a nonlinear dataset. However, we showed that this defect\ncan be transformed into a merit through combination with supervised\nclassifiers like SVM, where the tuning parameter\u2014such as a kernel\nparameter\u2014can be used to enhance sample discrimination in relation\nto the aim of the supervised task.\n\nWe expect PCA to exceed MCE on linear data, and for\npractical applications we accordingly suggest initial use of PCA\nin combination with differing normalizations; if the dataset\nshows subsequent resistance and nonlinearity (as in Fig. 4C), we\nrecommend structural exploration by means of MCE (as in Fig. 4A)\nand other NML techniques. Another solution could be the direct\nemployment of the MCE\u2013MCAP H2P approach, which in our results\nproved to be very powerful for visualization and unsupervised\nclassification. In particular, MCAP overcame AP in the clustering of\nelongated data in the bi-dimensional reduced space, but we expect\nthat, for regularly shaped clusters, AP might perform similarly or\nbetter.\n\ni537\n\nD\now\n\nnloaded from\n https://academ\n\nic.oup.com\n/bioinform\n\natics/article/26/18/i531/205645 by guest on 12 July 2022\n\n\n\n[10:53 28/8/2010 Bioinformatics-btq376.tex] Page: i538 i531\u2013i539\n\nC.V.Cannistraci et al.\n\nAlthough the MC principle provided a valuable framework for\nthe estimation of curvilinear distances in small, nonlinear, multi-\ndimensional datasets, its extension to large datasets needs careful\nconsideration and adaptation. In such extension, the MST-measure\nmay not be sufficient to estimate the distances over the manifold with\nadequate approximation. Large numbers of samples can cause the\noverestimation of sample distances over the MST, and these large\ndistances could prevail in magnitude over the shorter ones in the\nlow-dimensional representation. An idea for future developments\nis the extension of the MC approach to other ML algorithms. In\nthis perspective, an option for future investigation might consider\nthe minimum curvilinear LLE (MCLLE), in which neighbors are\nestimated by distances over the MST and not, as in classical LLE,\nby Euclidean distances (EDs). On the other hand, there might be\nadditional benefit in the use of re-sampling techniques to compute\nmore refined estimations of manifold topology, where one possible\nsolution would be to estimate pair-wise distances by bootstrapping\nsamples and/or features.\n\nTo the best of our knowledge, the current study is the first to\nderive unsupervised classification of pain onset from CSF proteomic\nprofiles, and this result could offer new insights for the future\ncharacterization of pain in molecular and systems biology.\n\nA final observation regards tissue embryological classification.\nPrompted by the improved accuracy here reported, we suggest that\nthe skin label might be reinterpreted as a mesodermal attribution. We\nalso conjecture that data nonlinearity could be completely addressed\nby methods for DR that more finely exploit the intrinsic patterns\nhidden in biological TF-network topology.\n\n5 METHODS\n\n5.1 Tuning stage: for addressing data nonlinearity\nIndex TE is estimated by means of LOOCV in order to prevent overfitting.\nHaving fixed the value of the free parameter k, the considered DR algorithm\nprovides a two-DR for each leave-one-out step, excluding one sample from\nthe training dataset in each round of LOOCV (nine samples in the dataset\nprovide nine DRs during the leave-one-out procedure), and then estimating\na proposed cluster validity measure (CVM) (Stein et al., 2003) in the 2D\nreduced space. The CVM is here used as a measure of separation between\nthe two classes C and P present in the training dataset. Higher CVM values\nmean better separation between the two considered groups in the 2D-reduced\nspace; in the absence of linear separation CVM provides value zero. Thus,\nfor every value of k an ensemble of CVMs is computed during LOOCV.\nThis ensemble is adopted to calculate the index TE in correspondence to any\nvalue of the free parameter k according to the following formula:\n\nTE\n(\nk\n)= mean\n\n(\nCVMs\n\n)\n\n1+SD(CVMs) .\n\nThe mean is divided by the SD (+1) of the CVMs. This estimation is\nused to measure the training optimality of the considered algorithm in\ncorrespondence to each value assumed by the free parameter k. For \u2018SD\u2019\nequal to zero, TE takes a value corresponding to the mean CVM. For \u2018SD\u2019\n>0, TE is penalized with respect to the mean CVM. In the absence of linear\nseparation, TE has zero value if the CVM has zero value for each of the\nLOOCV steps. The rationale is to select the parameter value that offers\nhigh-cluster separation (high-CVM mean value) and at the same time ensures\nhigh reliability and robustness (low-CVM SD) during the cross-validation\nprocedure. Details on CVM and accuracy computing, as well as details of\nthe toolbox used for implementation of LLE, LTSA and Isomap algorithms,\nare provided in Supplementary Data (paragraph 1).\n\n5.2 Validation Stage 2: prediction and classification of\npain subjects\n\nDuring the \u2018validation Stage 2\u2019, DR\u2014in a 2D space\u2014of the entire dataset\nwas obtained by exploiting the best parameter setting k =3, which was\nlearned for the LLE algorithm during the \u2018tuning stage\u2019. An ensemble of\ndecision boundary (DB) was subsequently obtained, by means of SVM and\nthe procedure based on the LOOCV (described above), using the same C\n(n=5, C1, \u2026, C5) and P (n=4, P1, \u2026, P4) samples as those previously\nemployed in the training of the \u2018tuning stage\u2019. The DB offering median\ndistance between the support vectors was designated as the decision rule.\n\n5.3 MCAP\nThe first step is to calculate a distance matrix (MC-matrix) as pair-wise\nsample distances over the MST, as computed by the Kruskal method in\nthe feature space (in our case, 2D-reduced space). For MST computation,\nwe suggest the use of a heuristic metric that we found fitted efficiently\nin combination with the message passing procedure run by AP over the\nMST. The suggested heuristic is the square root of the EDs between\nthe samples. This device attenuates the estimation of large distances\nand amplifies the estimation of short distances; consequently the device\nhelps to regularize the distances over the MST for the message passing\nprocedure. In the second step, AP is run assuming sample similarities\nequal to the negative values of the elements in the MC-matrix\u2014computed\nas previously described\u2014and tuning the preference parameter (Frey and\nDueck, 2007) to the value that offers two clusters as algorithm result.\nMatlab code at: https://sites.google.com/site/carlovittoriocannistraci/home\nhttp://www.mathworks.com/matlabcentral/ (tag: MC).\n\n5.4 MCE\nThe first step is to calculate a distance matrix (MC-matrix) as pair-wise\nsample distances over the MST as computed by the Kruskal method in the\nfeature space. To compute the MST in the feature space, we tested the ED\nand the correlation distance (CD) obtained as:\n\ncorr(x,y)=1\u2212corrperson(x,y)\nIn general, the two different distances provided comparable results. We\n\nused the CD in our computation, except for the analysis of dataset 2, in\nwhich the ED was preferred. In the second step we performed the embedding\ntransformation by performing the classical MDS of the MC-matrix. We also\ntested Sammon nonlinear MDS (S-MDS), but the result on our data, although\ncomparable, was less impressive. Matlab code is provided on the web sites\nindicated in Section 5.3.\n\nACKNOWLEDGEMENTS\nWe thank Ewa Aurelia Miendlarzewska for her generous assistance\nand for language revision and Sven Bergmann for precious\nsuggestions.\n\nFunding: Fondazione CARIPLO (NOBEL GuARD Project); MoH\nRF-FSR-2007-637144; Italian Interpolytechnic School of Doctorate\nSIPD (http://sipd.polito.it/) (to C.V.C.); US National Institute of\nMental Health and the King Abdullah University of Science and\nTechnology (grant MH062261 to T.R. and C.V.C.).\n\nConflict of Interest: none declared.\n\nREFERENCES\nBalasubramanian,M. and Schwartz,E.L. (2002) The Isomap algorithm and topological\n\nstability. Science, 295, 7.\n\ni538\n\nD\now\n\nnloaded from\n https://academ\n\nic.oup.com\n/bioinform\n\natics/article/26/18/i531/205645 by guest on 12 July 2022\n\n\n\n[10:53 28/8/2010 Bioinformatics-btq376.tex] Page: i539 i531\u2013i539\n\nNonlinear dimension reduction and clustering by Minimun Curvilinearity\n\nBaldi,P. and Brunak,S. (eds) (1998) Hybrid systems: hidden Markov models and neural\nnetworks. In Bioinformatics: The Machine Learning Approach. The MIT Press,\nCambridge, MA, USA.\n\nBaron,R. (2006) Mechanisms of disease: neuropathic pain\u2013a clinical perspective. Nat.\nClin. Pract. Neurol., 2, 95\u2013106.\n\nBogu\u00f1\u00e1,M. et al. (2009) Navigability of complex networks. Nat. Phys., 5, 74\u201380.\nCannistraci,C.V. et al. (2009) Median-modified Wiener filter provides efficient\n\ndenoising, preserving spot edge and morphology in 2-DE image processing.\nProteomics, 9, 4908\u20134919.\n\nConti,A. et al. (2005) Pigment epithelium-derived factor is differentially expressed in\nperipheral neuropathies. Proteomics, 5, 4558\u20134567.\n\nConti,A. et al. (2008) Differential expression of ceruloplasmin isoforms in the\ncerebrospinal fluid of Amyotrophic Lateral Sclerosis patients. Proteomics Clin.\nAppl., 2, 1628\u20131637.\n\nDorshkind,K. (2002) Multilineage development from adult bone marrow cells. Nat.\nImmunol., 3, 311\u2013313.\n\nFinnerup,N.B. and Jensen,T.S. (2006) Mechanisms of disease: mechanism-based\nclassification of neuropathic pain-a critical analysis. Nat. Clin. Pract. Neurol., 2,\n107\u2013115.\n\nFrey,B.J. and Dueck,D. (2007) Clustering by passing messages between data points.\nScience, 315, 972\u2013976.\n\nGhahramani,Z. (2004) Unsupervised learning. In Bousquet,O. et al. (eds) Advanced\nLectures on Machine Learning. Springer, Berlin, Germany, pp. 72\u2013112.\n\nLattner,A.D. et al. (2004) A combination of machine learning and image processing\ntechnologies for the classification of image regions. In Proceedings of the Adaptive\nMultimedia Retrieval (Lecture Notes in Computer Science, LNCS series). Springer,\nBerlin/Heidelberg, pp. 341\u2013351.\n\nLeone,M. et al. (2007) Clustering by soft-constraint affinity propagation: applications\nto gene-expression data. Bioinformatics, 23, 2708\u20132715.\n\nMartella,F. (2006) Classification of microarray data with factor mixture models.\nBioinformatics, 22, 202\u2013208.\n\nMeyer-Rosberg,K. et al. (2001) Peripheral neuropathic pain - a multidimensional burden\nfor patients. Eur. J. Pain, 5, 379\u2013389.\n\nPattini,L. et al. (2008) An integrated strategy in two-dimensional electrophoresis\nanalysis able to identify discriminants between different clinical conditions. Exp.\nBiol. Med., 233, 483\u2013491.\n\nRavasi,T. et al. (2010) An Atlas of Combinatorial Transcriptional Regulation in Mouse\nand Man. Cell, 140, 744\u2013752.\n\nRoweis,S.T. and Saul,L.K. (2000) Nonlinear dimensionality reduction by locally linear\nembedding. Science, 290, 2323\u20132326.\n\nSammon,J.W. (1969) A nonlinear mapping for data structure analysis. IEEE Trans.\nComput., 18, 401\u2013409.\n\nShawe-Taylor,J. and Cristianini,N. (2004) Kernel Methods for Pattern Analysis.\nCambridge University Press, New York, NY, USA.\n\nSmialowski,P. et al. (2009) Pitfalls of supervised feature selection. Bioinformatics, 26,\n440\u2013443.\n\nStein,B. et al. (2003) On cluster validity and the information need of users. In\nHanza,M.H. (ed.) Proceedings of the 3rd IASTED International Conference on\nArtificial Intelligence and Applications (AIA 03). ACTA Press, Benalmadena, Spain,\npp. 216\u2013221.\n\nTenenbaum,J.B. et al. (2000) A global geometric framework for nonlinear\ndimensionality reduction. Science, 290, 2319\u20132323.\n\nZhang,Z. and Zha,H. (2004) Principal manifolds and nonlinear dimensionality reduction\nvia local tangent space alignment. SIAM J. Sci. Comput., 26, 313\u2013338.\n\ni539\n\nD\now\n\nnloaded from\n https://academ\n\nic.oup.com\n/bioinform\n\natics/article/26/18/i531/205645 by guest on 12 July 2022\n\n\n", "status": 200, "abstract": "Motivation: Nonlinear small datasets, which are characterized bylow numbers of samples and very high numbers of measures, occurfrequently in computational biology, and pose problems in theirinvestigation. Unsupervised hybrid-two-phase (H2P) procedures\u2014speci\ufb01cally dimension reduction (DR), coupled with clustering\u2014providevaluableassistance,notonlyforunsuperviseddataclassi\ufb01cation, but also for visualization of the patterns hidden inhigh-dimensional feature space.Methods: \u2018Minimum Curvilinearity\u2019 (MC) is a principle that\u2014forsmall datasets\u2014suggests the approximation of curvilinear sampledistances in the feature space by pair-wise distances over theirminimum spanning tree (MST), and thus avoids the introduction ofany tuning parameter. MC is used to design two novel forms ofnonlinear machine learning (NML): Minimum Curvilinear embedding(MCE) for DR, and Minimum Curvilinear af\ufb01nity propagation (MCAP)for clustering.Results: Compared with several other unsupervised and supervisedalgorithms, MCE and MCAP, whether individually or combined inH2P, overcome the limits of classical approaches. High performancewas attained in the visualization and classi\ufb01cation of: (i) pain patients(proteomic measurements) in peripheral neuropathy; (ii) human organtissues (genomic transcription factor measurements) on the basis oftheir embryological origin.Conclusion:MC provides a valuable framework to estimatenonlinear distances in small datasets. Its extension to large datasetsis pre\ufb01gured for novel NMLs. Classi\ufb01cation of neuropathic painby proteomic pro\ufb01les offers new insights for future molecular andsystems biology characterization of pain. Improvements in tissueembryological classi\ufb01cation re\ufb01ne results obtained in an earlierstudy, and suggest a possible reinterpretation of skin attribution asmesodermal.Availability: https://sites.google.com/site/carlovittoriocannistraci/homeContact: kalokagathos.agon@gmail.com; massimo.alessio@hsr.itSupplementary information: Supplementary data are available atBioinformatics online."}